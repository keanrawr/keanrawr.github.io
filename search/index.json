[{"content":"Why do this? One of the most common pieces of advice that data science newbies get is to learn web scraping, this is usually acompanied by an argument like: \u0026ldquo;WS is a really important skill for data science, since being able to scrape your own dataset from the web comes in handy\u0026rdquo;.\nAs a person who dives head first into anything that\u0026rsquo;s deemed \u0026ldquo;highly valuable\u0026rdquo; I went head first into learning web scraping. However, after years in my data science journey, rarely have I had to scrape my own datasets. And this is to be expected, most companies can\u0026rsquo;t even handle the amount of existing data they have at hand, or wouldn\u0026rsquo;t mind paying for some API service or data broker to provide additional datasets.\nThis doesn\u0026rsquo;t mean I think learning web scraping isn\u0026rsquo;t useful, on the contrary, I find WS to be important, but not because you\u0026rsquo;ll have to scrape your datasets. I think that WS is one of the easiest ways to dip your toe into how the web works. As a data scientist you\u0026rsquo;ll eventually find yourself in contact with some form of web service, it could be some endpoint from your company, or a third-party vendor, that provides some data you need, or when putting some model in production. Regardless of the way, shape or form that this interactions comes, knowing your way around what terms like \u0026ldquo;request\u0026rdquo;, \u0026ldquo;authentication\u0026rdquo; or \u0026ldquo;body\u0026rdquo; mean, will make these much easier.\nThe previous being said, we\u0026rsquo;ll go through a simple and fun use case that serves as a light introduction to WS, and as a side effect provides you with a bunch of emojis for your slack workspace!\nWhat is web scraping? There have been many blog posts written on the meaning of WS, so I won\u0026rsquo;t waste too much time defining it.\nWeb scraping is the process of requesting a web page containing data you\u0026rsquo;re interested in, and parsing out said information for some purpose, e.g., downstream processing, or running a text analysis.\nWhat we\u0026rsquo;ll be doing We\u0026rsquo;ll be using the rvest R package to scrape slackmojis.com for the blob cats category. My employer\u0026rsquo;s slack workspace is lacking on some meow action. We\u0026rsquo;ll be following these rough steps:\nIdentify the target URL(s) Identify the selector to the emojis Save the emojis locally (Optional) Add them to a slack workspace To follow along you\u0026rsquo;ll need:\nA working installation of R The rvest and stringr R packages (Optionally) the Rstudio IDE A browser with developer tools (e.g., Chrome, Firefox, Edge, Brave) Idetify the target URL(s) This one\u0026rsquo;s kind of a freebie right? We already said that we\u0026rsquo;ll be scraping https://slackmojis.com/categories/25-blob-cats-emojis for meow emojis right? Well, we can still do a little more. If we look closely at the page, there\u0026rsquo;s a \u0026ldquo;See More Blob Cats Emojis\u0026rdquo; button.\nSee more If we click it we are taken to a page with more emojis, looking at the address bar we notice that it\u0026rsquo;s essentially the same address as before, only with some extra characters: ?page=1. This is called a query string, and it\u0026rsquo;s something that we use to tell a web application more about the content we\u0026rsquo;re requesting through some parameters and values, the page number in this case.\nQuery string page 1 Query parameters are specified in the query string, we use the ? character to separate the URL from the query string. The structure of a query string follows this format: param1=value1\u0026amp;param2=value2\u0026amp;....\nüìù This is a very simple example where we aren\u0026rsquo;t encoding special characters like a whitespace or a question mark in the parameters or values from our query string.\nWhy is this useful? well, first of all, you\u0026rsquo;re welcome for some useful knowledge. Second, in cases like this we can leverage this information to make our WS adventure easier.\nEach emoji page contains a 200x5 grid, with this many emojis per page we only need two pages to get all the images we\u0026rsquo;re interested in, something we can manually handle, but imagine that each page contained only 10 emojis, if that were the case, changing the url manually would be really annoying. Let\u0026rsquo;s play with our query parameter, if we set page=0 and hit \u0026ldquo;go\u0026rdquo; in our browser we get\u0026hellip;\nQuery string page 0 The first page! Now we know that we have the ability to programmatically iterate through these pages. This may not be very useful for this particular case, but it might come in handy in other cases.\nNow that we\u0026rsquo;ve identified what our target URLs look like we can write some initial R code to catch this behavior.\nIdentify the selector to the emojis I like to think that selectors are the cornerstone of web scraping, it\u0026rsquo;s how we tell rverst (or any other WS framework) what elements of the page we actually want. To give a brief explanation on what selectors are it\u0026rsquo;s worth exploring what a web page is.\nAll web pages are HTML under the hood, Whenever you visit a web page your browser parses the raw HTML, and with the help of CSS and JavaScript turns it into whatever\u0026rsquo;s in front of you. You might\u0026rsquo;ve entered \u0026ldquo;source mode\u0026rdquo; by accident at one point, since most chromium-based browsers like chrome or modern edge, have this capability.\nSource code page example Parsed page example Even though we can use the source code version to find what we need, that\u0026rsquo;s somewhat of a pain, we\u0026rsquo;d need to have the ability to stare at a wall of symbols and parse it in our heads, like Neo and his friends do in the Matrix movies. Fortunately for us, our browser provides us with the developer tools. It might be different depending your browser, but in general you can access the developer tools using the F12 button in your keyboard.\nDeveloper tools With our handy developer tools we can go ahead and find our selector. First of all, you\u0026rsquo;ll want to be in the \u0026ldquo;Inspector\u0026rdquo; tab of your developer tools. Now, if we hover over some of the HTML we\u0026rsquo;ll see the corresponding content get highlighted on the page.\nHighlighted content We can see that the\u0026rsquo;s some funny looking stuff like \u0026lt;ul class=\u0026quot;groups\u0026quot;\u0026gt;, these things are called tags, and they\u0026rsquo;re the building blocks of an HTML document, if you\u0026rsquo;ve ever seen an XML file these will look familiar. Tags can be nested, and in this case that\u0026rsquo;s what we\u0026rsquo;re seeing. Tags can have attributes, some tags have required attributes to do their thing, and some attributes can be part of any tag, the class and id attributes are examples of this, these attributes have special selector characters, prefixing a class with a dot (e.g., .my-class) selects all tags with the my-class class, to select a tag using its id attribute simple use # as a prefix. Classes applied to a tag are separated by spaces.\nNow, like modern computer-savy Hansel and Grettel we\u0026rsquo;ll follow the HTML tags breadcrumbs to what we need. If we unnest tags following the trail to the first emoji we\u0026rsquo;ll get to this:\nFollow the breadcrumbs We\u0026rsquo;ve found some interesting stuff here, apparently there are classes like \u0026ldquo;emojis\u0026rdquo; and \u0026ldquo;downloader\u0026rdquo; that can be used to target a single emoji. What we\u0026rsquo;re intesrested in is in the img tag, which has the src attribute, this attribute is used to specify the URL to the actual image. If we copy the src value and pase it on the address bar we get this:\nEmoji URL Alright! Now that we know what we need let\u0026rsquo;s code it up. We\u0026rsquo;ll be using the read_html function to read the page, html_elements to extract elements using selectors, and html_attr to extract the emoji url.\nIf we execute this code snippet we can verify the result we\u0026rsquo;re getting is what\u0026rsquo;s expected.\nEmoji links üìù We could also leverage a browser extension like CSS selector, but I find them to be redundant. There are cases when your target might be so complex that using an extension to find the correct selector is beneficial, but in my experience, those cases are the exceptions rather than the rule\nSave the emojis locally The only missing puzzle piece is a way to save our emoji files locally. Fortunatelly for us, R has a built-in download.file method that we can use. This method only requires the URL and the destination for the file to be saved, we already have the URLs, we now need to define the destination. Since I\u0026rsquo;m really bad at regex and R\u0026rsquo;s regex support doesn\u0026rsquo;t make it any easier for me, we\u0026rsquo;ll use the stringr package.\nNow we can put everything together!\nAnd that\u0026rsquo;s it! With less than 30 lines of R code we were able to download a bunch of meow emojis that are ready to be dropped in some slack converstations with my coworkers.\n(Optional) Add them to a slack workspace Once we have all our files downloaded we can add them to our slack workspace. Slack doesn\u0026rsquo;t provide support for bulk uploads out of the box, but we can use the Neutral Face Emoji Tools browser extension to help with that. If you\u0026rsquo;re going to be using the extension, I recommend to not upload more than 20 emojis at a time, or you\u0026rsquo;ll hit a limit on the number of emojis to upload. Any emojis that don\u0026rsquo;t go through, can be easily identified and retry to upload.\nFinal thoughs Hope this small tutorial helps anyone other than future me for reference on downloading emoji packs. I\u0026rsquo;m working on putting together a more complex use case of WS to scrape a dataset to use in an actual analysis. Until then, see you around.\n","date":"2023-03-19T00:00:00Z","image":"https://keanrawr.github.io/p/scrape-emojis-for-slack/banner_hu8c5b92596334b1bbc9202462769c8dfa_166775_120x120_fill_q75_box_smart1.jpg","permalink":"https://keanrawr.github.io/p/scrape-emojis-for-slack/","title":"Scrape emojis for slack"},{"content":"Why you need to track experiments As data scientists or Machine Learning engineers one of the most overlook things is \u0026ldquo;model lineage\u0026rdquo;, that is, being able to track the data and hyper parameters used to produce a model. This helps ensure reproducibility, which is really important for a number of reasons.\nmlflow isn\u0026rsquo;t the only tool that helps us track experiments, but is the one I like the most, other tools for experiment tracking are: comet.ml, tensorboard, dvc. All of the previous list, minus DVC, have a GUI to explore results, DVC is more like a git complement and is my favorite from the list.\nPrerequisites For this build you\u0026rsquo;ll need:\nA raspberry pi with Raspberry Pi OS installed Docker installed in your raspberry pi. It\u0026rsquo;s really easy to set up, follow the official Docker install guide for debian Python3 installed in your raspberry pi *python virtual env to isolate the requirements for the server *an aws account * optional requirements\nWhat we\u0026rsquo;ll be setting up There are three main parts to an mlflow server:\nThe server UI: This is the flask web application that lets you interact with the experiments that have been run. Backend storage: Place where the experiments, runs, metrics and parameters are stored. It can be local file storage, but in order to use the mlflow model registry we need to use a relational database, in our case that\u0026rsquo;s postgres. Artifact storage: We need a place for artifacts, such as the serialized model, or a plot that you want to save. This can also be locally stored, but we\u0026rsquo;ll see an optional aws s3 storage solution. In the end this is what we\u0026rsquo;ll be building.\ndiagram Setting up backend storage Docker is a great tool because it lets us deploy a number of tools fast and easy, plus it has a lot of official images, one of which is postgres. To run a postgres docker container there are a couple of things that we need to define, the basic syntax is:\ndocker run -d --name container-name \\ -e POSTGRES_PASSWORD=mysecretpassword \\ postgres The only required environment variable is POSTGRES_PASSWORD, the default user name is \u0026ldquo;postgres\u0026rdquo; and the default database name is whatever the user is, you can learn more about the docker image in the official documentation. This is good enough to get you started, but we suggest you declare at least a couple more arguments:\ndocker run -d -p 5432:5432 \\ --name mlflow-backend \\ --restart unless-stopped \\ -e POSTGRES_USER=mlflow \\ -e POSTGRES_PASSWORD=supersecretpassword \\ -e POSTGRES_DB=mlflow \\ postgres:14 Let\u0026rsquo;s break the command down:\n-d runs the container in detached mode -p specifies port mappings, it\u0026rsquo;s always a good practice to specify this in case you want to specify a different port --name gives our docker image a name to find it when using the docker ps command --restart unless-stopped makes sure that our container is always up, even when rebooting the pi POSTGRES_USER, POSTGRES_PASSWORD and POSTGRES_DB are all postgres related variables. Make sure to choose a good password. postgres:14 the name of the image to use, it\u0026rsquo;s always a good practice to specify the tag you want to use, in case the latest version of the image has breaking changes. (Optional) Set up s3 artifact storage This is an optional step to use s3 as our artifact storage, we\u0026rsquo;ll be setting up 3 things:\nS3 bucket aws user with access to the bucket aws cli credentials in the raspberry pi Creating s3 bucket In your aws console go to the s3 console, you can use the search bar or in the \u0026ldquo;All services\u0026rdquo; part of the console. Once in the s3 console click on the \u0026ldquo;Create bucket\u0026rdquo; button.\ns3-console Choose a name and region for your bucket. For the bucket name choose a descriptive name, bucket names need to be unique across aws regions, so you might need to give it a couple of tries. For the region, choose the closest one to you. Leave everything else with the default settings and click on \u0026ldquo;Create bucket\u0026rdquo; at the bottom of the page.\ns3-create Note: You don\u0026rsquo;t need a new bucket for this, in my case I have a bucket that I use for miscellaneous things and just use a different root for each use case.\nCreate an aws user to access the bucket Navigate to the IAM console, in the side menu there\u0026rsquo;s an \u0026ldquo;Users\u0026rdquo; menu item. Once there click on the \u0026ldquo;Add users\u0026rdquo; button.\niam-console Give your user a memorable name and only enable programmatic access. Click on the \u0026ldquo;Next: Permissions\u0026rdquo; button.\niam-step1 Next we\u0026rsquo;ll setup the user permissions, in the second step clik on \u0026ldquo;Attach existing policies directly\u0026rdquo; to choose a policy to attach, there is no policy yet, so we\u0026rsquo;ll need to create a new one. To create it click on the \u0026ldquo;Create policy\u0026rdquo; button, this opens a new tab.\niam-step2 In the new tab that was opened click on the \u0026ldquo;JSON\u0026rdquo; tab to define the new policy using JSON. Paste the following text and don\u0026rsquo;t forget to change the bucket name for your bucket name:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::YOUR_BUCKET_NAME/*\u0026#34;, \u0026#34;arn:aws:s3:::YOUR_BUCKET_NAME\u0026#34; ] } ] } This policy ensures that the user only has access to the bucket of your choosing.\niam-step3 Click on \u0026ldquo;Next: Tags\u0026rdquo; and optionally add tags to identify the policy. After That click on \u0026ldquo;Next: Review\u0026rdquo; to finish configuring the policy. Choose a name and optionally add a description to your new policy.\niam-step4 Click on \u0026ldquo;Create policy\u0026rdquo; to finish creating the policy. You can now go back to the user creation tab. Click on the refresh button and use the filter to look for your newly created policy. Select it and click the \u0026ldquo;Next\u0026rdquo; button until you create your new user. Once the user has been created you\u0026rsquo;ll be able to copy the access key and the secret access key only this once.\niam-user-secrets Raspberry Pi aws CLI credentials Setting up the aws CLI credentials is very simple, you don\u0026rsquo;t even need to install the cli to complete this step. Logged into your raspberry pi create a directory in your home directory with the name .aws, then navigate to it and create a file named config and another one named credentials.\n# config file [default] region = us-west-2 Change the region value for the region of your bucket.\n# credentials file [default] aws_access_key_id = USER_ACCESS_KEY aws_secret_access_key = USER_SECRET_ACCESS_KEY Insert the credentials for the created aws user.\nSetting up mlflow server We\u0026rsquo;ll be using a python virtual environment to isolate our mlflow dependencies and other requirements. But you can always choose to not use a virtual environment.\nLogged into your raspberry pi run the following commands\n# create a diretory for our instalation mkdir mlflow cd mlflow # create virtual environment and activate it python3 -m venv venv source venv/bin/activate # install dependencies sudo apt update sudo apt install awscli pip install mlflow[extras] psycopg2-binary boto3 pip install --upgrade awscli Now we can run the mlflow server. For this example we\u0026rsquo;ll be writing the postgres credentials in plain text, but this is something that could be setup as environment variables. Run only one of these commands:\n# local storage for artifacts nohup mlflow server \\ --backend-store-uri postgresql://mlflow:password@localhost:5432/mlflow \\ --host 127.0.0.1 --port 5000 \u0026amp; # s3 artifact starage nohup mlflow server \\ --backend-store-uri postgresql://mlflow:password@localhost:5432/mlflow \\ --default-artifact-root s3://some-bucket-name \\ --host 127.0.0.1 --port 5000 \u0026amp; # s3 artifact starage with customs root nohup mlflow server \\ --backend-store-uri postgresql://mlflow:password@localhost:5432/mlflow \\ --default-artifact-root s3://some-bucket-name/custom-artifact-root \\ --host 127.0.0.1 --port 5000 \u0026amp; Let\u0026rsquo;s break the command down:\nnohup runs our command in the backgroug --backend-store-uri is the URI to our docker postgres container --default-artifact-root in case we\u0026rsquo;re using s3 as artifact storage, we specify the bucket and folder that we want mlflow to use as the root path for all artifacts --host and --port specify that we\u0026rsquo;re using localhost and the 5000 port, it\u0026rsquo;s a good practice to specify it If you use your browser to navigate to your raspberry pi\u0026rsquo;s IP and port (e.g. http://192.168.100.15:5000), you should see the mlflow UI (assuming that you have have the port open in your Pi). If your Pi isn\u0026rsquo;t onfigured to allow connections on the specified port. Allowing trafic on ports is out of the scope of this post, but there are great resources out there on how to open ports for connections.\nSetting up nginx Finally we\u0026rsquo;ll setup nginx as a reverse proxy to handle traffic to our mlflow server using http basic authentication. Note that we\u0026rsquo;re assuming you haven\u0026rsquo;t setup a different web server like apache, tomcat or lighttpd. To install nginx run the following commands:\nsudo apt update sudo apt install nginx sudo apt install apache2-utils # verify nginx is running sudo systemctl restart nginx # create an user for http auth sudo htpasswd -c /etc/nginx/.htpasswd someuser # configure nginx sudo unlink /etc/nginx/sites-enabled/default cd /etc/nginx/sites-available sudo nano mlflow.conf For the confifuration paste the following:\nserver { listen 80; listen [::]:80; # change server_name to your raspberry pi\u0026#39;s IP server_name 192.168.x.x; # if you have a local DNS setup you can use a custom domain # server_name my_local_dns_domain.com; access_log /var/log/nginx/mlflow-access.log; error_log /var/log/nginx/mlflow-error.log; location / { proxy_pass http://127.0.0.1:5000; auth_basic \u0026#34;Restricted Content\u0026#34;; auth_basic_user_file /etc/nginx/.htpasswd; } } Final commands to check the configuration was done correctly and reload nginx:\nsudo ln -s /etc/nginx/sites-available/mlflow.conf /etc/nginx/sites-enabled/mlflow.conf # test configuration sudo nginx -t # restart nginx sudo systemctl restart nginx Starting mlflow on raspberry pi reboot Our final step is to make sure that our mlflow server is up even after we reboot our Pi. We can do that with a simple shell script and crontab.\n# navigate to our mlflow directory cd ~/mlflow nano start-mlflow.sh Paste the the following contents in the script:\n#!/bin/bash HERE=$( cd \u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34;)\u0026#34; ; pwd -P ) cd $HERE source venv/bin/activate # use the version of the mlflow command that you chose nohup mlflow server \\ --backend-store-uri postgresql://mlflow:password@localhost:5432/mlflow \\ --default-artifact-root s3://some-bucket-name/custom-artifact-root \\ --host 127.0.0.1 --port 5000 \u0026amp; Use Ctrl + O to save and Ctrl + X to exit.\nchmod +x start-mlflow.sh crontab -e Paste the following line at the end of the file:\n# mlflow tracking server @reboot ~/mlflow/start-mlflow.sh And all done! üéâ Your mlflow tracking server is now live. You can go to your raspberry pi\u0026rsquo;s IP (or domain if you setup a local DNS domain like I did) and you should see your tracking server. It\u0026rsquo;ll ask you for the username you defined in the nginx configuration part.\nmlflow-login mlflow-server ","date":"2022-01-04T00:00:00Z","image":"https://keanrawr.github.io/p/hosting-mlflow-on-raspberry-pi-behind-nginx/banner_hu1e9798d7e5cf567e46abb69a070566d3_315645_120x120_fill_box_smart1_3.png","permalink":"https://keanrawr.github.io/p/hosting-mlflow-on-raspberry-pi-behind-nginx/","title":"Hosting mlflow on Raspberry Pi Behind nginx"},{"content":"Hello world!\nThis is the first official post on my new blog! In this blog I\u0026rsquo;ll talk about data projects that I develop on my free time, and some of the techniques that I\u0026rsquo;ve learned over my professional career.\nA little about me: I\u0026rsquo;m a data scientist that loves to write code, I love writing in R but focus mainly on python for my professional work. I love playing basketball, not actually a real hard-core NBA fan, but I might start getting more interested in the league to do some fantasy league analysis.\nI\u0026rsquo;m also a big League of Legends fan, haven\u0026rsquo;t really played much since graduating from university.\nHope that anyone that reads find some interesting material.\n","date":"2021-07-20T00:00:00Z","image":"https://keanrawr.github.io/p/first-post/kean-first-post_hue776a334b902d7828bf5c7a8aa11ccb3_204238_120x120_fill_q75_box_smart1.jpg","permalink":"https://keanrawr.github.io/p/first-post/","title":"First Post"}]